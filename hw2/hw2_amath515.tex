\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}

\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%

\usepackage{graphicx, url}
\graphicspath{figures/}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass}
\rhead{\hmwkTitle}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Homework\ \#2}
\newcommand{\hmwkDueDate}{February 11, 2019}
\newcommand{\hmwkClass}{AMATH 515 (Optimization)}
\newcommand{\hmwkClassTime}{Section A}
\newcommand{\hmwkClassInstructor}{Prof. Alexander Aravkin}
\newcommand{\hmwkAuthorName}{\textbf{Alexey Sholokhov}}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 23:59}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

% Different math operators
\DeclareMathOperator{\dom}{Dom}
\DeclareMathOperator{\diag}{Diag}
\DeclareMathOperator{\prox}{prox}
\DeclareMathOperator*{\proj}{proj}
\DeclareMathOperator*{\sign}{sign}

%% \mathbb symbols
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\A}{\mathbb{A}}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\X}{\mathbb{X}}
\DeclareMathOperator{\N}{\mathbb{N}}
\DeclareMathOperator{\Q}{\mathbb{Q}}

%% \mathcal symbols
\DeclareMathOperator{\RR}{\mathcal{R}}
\DeclareMathOperator{\PP}{\mathcal{P}}
\DeclareMathOperator{\NN}{\mathcal{N}}
\DeclareMathOperator{\CC}{\mathcal{C}}



\begin{document}

\maketitle

\pagebreak

    
\begin{homeworkProblem}
    The following problems concern strong convexity:
    
    \begin{itemize}
    \item[(a)] 	Show that a $C^1$-smooth function $f$ is $\alpha$-strongly convex if and only if the function $g(x) = f (x) - \frac{\alpha}{2} \|x\|^2$ is convex.
    \medskip


    \item[(b)] 	Suppose that $f$ is $\alpha$-strongly convex. Show that any minimizer of $f$ is unique. 
    \medskip

    \item[(c)] 	Suppose that $f$ is $\alpha$-strongly convex with minimizer $x^*$. Show that 
    \[
        f(x) \geq f(x^*) + \frac{\alpha}{2} \|x-x^*\|^2. 
    \]
\end{itemize}
    
\solution
        
    \begin{itemize}
        \item[a)] Suppose that $f$ is $\alpha$-strongly convex. Then we have a sequence of equivalent transitions:
        \begin{equation}
            \begin{split}
                f(y) \geq & f(x) + \nabla f(x)^T(y-x) + \frac{\alpha}{2}\|x-y\|^2  = \\
                = & f(x) + \nabla f(x)^T(y-x) + \frac{\alpha}{2}\left(\|y\|^2 - 2x^Ty + \|x\|^2 \right) = \\
                = & f(x) + \nabla f(x)^T(y-x) - \alpha x^T(y-x) + \frac{\alpha}{2}\|y\|^2 - \frac{\alpha}{2}\|x\|^2 \\
            \end{split}
        \end{equation}
            so it is equivalent to:
            \[
                f(y) - \frac{\alpha}{2}\|y\|^2 \geq f(x) - \frac{\alpha}{2}\|x\|^2 + (\nabla f(x) - \alpha x)^T(y-x) 
            \]
            which means that $f(x) - \frac{\alpha}{2}\|x\|^2$ is convex. \qed
            \item[b)] Suppose that there are two different minimizers: $x_1$ and $x_2$:\\
            \(x_1\text{ -- minimizer } \implies \forall y\: f(y) \geq f(x_1) + \frac{\alpha}{2}\|y - x_1\| \implies f(x_2) \geq f(x_1) + \frac{\alpha}{2}\underbrace{\|x_2 - x_1\|^2}_{> 0} > f(x_1) \) \\ which means that $x_2$ is not a minimizer. Contradiction. \qed
        \item[c)] By definition: \(f(x) \geq f(x^*) + \nabla \underbrace{f(x^*)^T}_{=0}(x-x^*) + \frac{\alpha}{2}\|x - x^*\|^2 = f(x^*) + \frac{\alpha}{2}\|x - x^*\|^2\) \qed
    \end{itemize}    

\end{homeworkProblem}


\begin{homeworkProblem}

Suppose $f$ is convex.
\begin{enumerate}
%\item Prove $\mbox{prox}_f(x) + \mbox{prox}_{f^*}(x) = x$.

\item Prove that $f_t$ is convex.  
\bigskip

\item Prove that $\prox_{t f}$ is a single-valued mapping. 
\bigskip

\item Compute $\prox_{t f}$ and $f_t$, where $f(x) = \|x\|_1$. 
%One way to do this is to consider the scalar case, and explicitly compute $f'(t)$ 
%\[
%f'_t(x) = \frac{1}{t} (x-\prox_{t f}(x)).
%\]
%Now, just find a function that has this derivative. 
\bigskip
\item Compute $\prox_{t f}$ and $f_t$ for $f = \delta_{\mathbb{B}_{\infty}}(x)$, 
where $\mathbb{B}_\infty = [-1,1]^n$.

\bigskip
\end{enumerate}
\newpage

\solution

\begin{itemize}
    \item[a)] First, we know that $\|*\|^2$ is a convex function. The sum of two convex functions is a convex function, so $\frac{1}{2t}\|x - y\|^2 + f(x)$ is also a convex function. Finally, we know (see the part 5 of lecture notes) that the partially minimized convex function is a convex function. Hence, $f_t$ is convex. \qed
    \item[b)] Note that $g(x, y) = f(x) + \frac{1}{2t}\|x - y\|^2$ is a strongly convex function as a sum convex and strongly convex: $f$ is convex and the norm is strongly convex. The later is because \(\|x-y\|^2 = (x-y)^TI(x-y)\), so \(\nabla^2\|x-y\|^2 = 2I \succ 0 \). So, according to 2b, $g(x)$ has a unique minimizer, which means that $\prox_{tf} = \arg \min_y g(x, y)$ is a single-value function. \qed
    \item[c)]
    \[
        \prox_{tf}(y) = \arg \min_x \frac{1}{2t}\|x - y\|_2^2 + \|x\|_1
    \]
    
    According to the optimality criteria for convex functions:
    \[
        \frac{x-y}{t} \in -\partial\|x\|_1
    \]
    \begin{equation} 
    \begin{split}
        x_j > 0, & y_j - x_j = -t \implies x_j = y_j + t,\, y_j < -t \\
        x_j < 0, & y_j - x_j = +t \implies x_j = y_j - t, \, y_j > t \\
        x_j = 0 & \, \implies x_j = 0,\, y_j \in [-t, t]  
    \end{split}
    \end{equation}
    or in the more compact form: 
    \[
        \prox_{tf}(y) = (y-t)_+ - (-y-t)_+
    \]
    Where $\text{(  )}_+$ is a positive slice. Consequently 
    \[
        f_t(y) = \frac{1}{2t}\|(y-t)_+ - (-y-t)_+ - y\| + \|(y-t)_+ - (-y-t)_+\|_1
    \]
    \qed
    \item[d)] \[
        \prox_{tf}(y) = \arg \min_x \frac{1}{2t}\|x - y\|^2 + \delta_{B_{\infty}}(x) = \arg \min_{x \in B_\infty}\|x-y\|^2 = \proj_{B_\infty}({y}) = \max(-1, \min(1, y))
    \]
    where $\max$ and $\min$ are element wise maximum and minimum respectively. Also 
    
    \[
        f_t(y) = \min_{x} \frac{1}{2t}\|x - y\|^2 + \delta_{B_\infty}(x) = \frac{1}{2t}\|\max(-1, \min(1, y)) - y\|^2
    \]
    
    
    \qed
    
    
\end{itemize} 

\end{homeworkProblem}

\begin{homeworkProblem}
More prox identities. 
\vskip 16pt
\begin{enumerate}
%\item Prove $\mbox{prox}_f(x) + \mbox{prox}_{f^*}(x) = x$.

\item Suppose $f$ is convex and let $g(x) = f(x) + \frac{1}{2}\|x-x_0\|^2$. 
Find formulas for $\prox_{t g}$ and $g_t$ in terms of $\prox_{t f}$ and $f_t$.
\bigskip

\item The elastic net penalty is used to detect groups of correlated predictors:
\[
g(x) = \beta \|x\|_1 + (1-\beta) \frac{1}{2}\|x\|^2, \quad \beta \in (0,1).
\] 
Write down the formula for $\prox_{t g}$ and $g_t$.
\bigskip

\item Let $f(x) = \frac{1}{2}\|Cx\|^2$. Write $\prox_{t f}(y)$ in closed form.

\bigskip

\item Let $f(x) = \|x\|_2$. Write $\prox_{tf}(y)$ in closed form.

\end{enumerate}

\solution

\begin{enumerate}
    \item[a)] Let \(g(x) = f(x) + \frac{1}{2}\|x - x_0\|^2 \)
    
    \begin{equation}
        \begin{split}
            \prox_{tg}(x) =& \arg \min_{y} \frac{1}{2t}\|x - y\|^2 + f(y) + \frac{1}{2}\|y - x_0\| = \\
            = & \arg \min_y f(y) + \frac{1}{2t}\left(x^Tx - 2x^Ty + y^Ty\right) + \frac{1}{2}\left(y^Ty - 2x_0^Ty + x_0^Tx_0 \right) = \\
            = & \arg \min_y f(y) + \frac{1}{2}
            \left(y^Ty\left(\frac{1+t}{1}\right) - 2y^T\left(\frac{1}{t}x+x_0\right)+\frac{1}{t}x^Tx + x^T_0x_0\right) = \\
            = & \arg \min_{y} f(y) + \frac{1}{2\underbrace{\frac{t}{1+t}}_{:= \lambda}}\left(y^Ty - 2y^T\left(\frac{1}{t+1}x + \frac{t}{t+1}x_0\right) + \underbrace{\frac{1}{t+1}x^Tx + \frac{t}{t+1}x_0^Tx_0}_{\text{does not depend on y}} \right)
        \end{split}
    \end{equation}
    Note that the last term does not depend on $y$, so it does not affect \(\arg \min_y\), hence, we can replace it with something else, which also does not depend on $y$:
    \begin{equation}
        \begin{split}
            = & \arg \min_y f(y) + \frac{1}{2\lambda}\left(y^Ty - 2y^T\left(\frac{\lambda}{t}x + \lambda x_0\right) + \underbrace{\|\frac{\lambda}{t}x + \lambda x_0\|^2}_{\text{this also does not depend on }y}\right)  = \\
            = & \arg \min_y f(y) + \frac{1}{2\lambda}\|y - \frac{\lambda}{t}x - \lambda x_0 \|^2 = \prox_{\lambda f}\left(\frac{\lambda}{t}x +\lambda x_0 \right)\text{, where } \lambda = \frac{t}{1+t}
        \end{split}
    \end{equation} 
    
    For $g_t(x)$ we have:
    
    \begin{equation}
        \begin{split}
            g_t(x) =& \min_{y} \frac{1}{2t}\|x - y\|^2 + f(y) + \frac{1}{2}\|y - x_0\| = \\
            = & \min_{y} f(y) + \frac{1}{2\underbrace{\frac{t}{1+t}}_{:= \lambda}}\left(y^Ty - 2y^T\left(\frac{1}{t+1}x + \frac{t}{t+1}x_0\right) + \frac{1}{t+1}x^Tx + \frac{t}{t+1}x_0^Tx_0 \right) = \\
            = & \min_y f(y) + \frac{1}{2\lambda}\|y - \frac{\lambda}{t}x - \lambda x_0 \|^2 - \|\frac{\lambda}{t}x + \lambda x_0\|^2 + \frac{\lambda}{t}x^Tx + \lambda x_0^Tx_0 = \\
            = & f_\lambda\left(\frac{\lambda}{t}x + \lambda x_0\right) - \|\frac{\lambda}{t}x + \lambda x_0\|^2 + \frac{\lambda}{t}x^Tx + \lambda x_0^Tx_0
        \end{split}
    \end{equation}
     
    \qed
    
    \item[b)] \[ \prox_{tg}(y) = \arg \min_x \beta\|x\|_1 + (1 - \beta)\frac{1}{2}\|x\|^2 + \frac{1}{2t}\|x - y\|^2 \]
    We use the sub-differential criterion for optimality:
    
    \begin{equation}
        \begin{split}
            0 \in \partial\beta |x_i| + & (1 - \beta)x_i + \frac{1}{t}(x_i - y_i) \\
            \text{1) } x_i < 0 \implies & t\beta = (1+t(1-\beta))x_i - y_i \\
            & \frac{t\beta+y_i}{1 + t(1 - \beta)} = x_i > 0 \implies y_i < -t\beta\\
            \text{2) } x_i > 0 \implies & -t\beta = (1+t(1-\beta))x_i - y_i \\
            & \frac{y_i-t\beta}{1+t(1-\beta)} = x_i > 0 \implies y_i > t\beta\\
            \text{3) } x_i = 0 \implies & y_i \in [-t\beta;\, t\beta] 
        \end{split}
    \end{equation}
    Summing up:
    \begin{equation}
        [\prox_{tg}(y)]_i = \begin{cases}
            \frac{y_i + t\beta}{1 + t(1 - \beta)} & y_i < -t\beta\\
            \frac{y_i - t\beta}{1+t(1-\beta)} & y_i > t\beta \\
            0 & y_i \in [-t\beta;\,t\beta]
        \end{cases}
    \end{equation}
    
    As we know the $\prox_{tg}$ we can define $g_t(y)$ as
    
    \[
        g_t(y) =\beta\|x^*\|_1 + (1 - \beta)\frac{1}{2}\|x^*\|^2 + \frac{1}{2t}\|x^* - y\|^2 \text{ where } x^* = \prox_{tg}(y)
    \] 
    
    \qed
    \item[c)] As the function under the argmin is convex and smooth, we will use the first order optimality criteria:
        \begin{equation}
            \begin{split}
                f(x) = & \frac{1}{2}\|Cx\|^2 \\
                \prox_{tf}(y) = & \arg \min_x \frac{1}{2t}\|x - y\|^2 + \frac{1}{2}\|Cx\|^2 \\
                \nabla  = & \frac{1}{t}(x-y)+C^TCx = 0\\
                & (I + C^TC)x = y\\
                \prox_{tf}(y) = & x = (I + tC^TC)^{-1}y
            \end{split}
        \end{equation}
        \qed
        
    \item[d)] 
    \[
        \prox_{tf}(y) = \arg \min_x \frac{1}{2t}\|x - y\|^2 + \|x\|_2
    \]
    As the function under the argmin is convex and smooth, we will use the first order optimality criteria:
    
    \begin{equation}
        \begin{split}
            \nabla = & \; \frac{1}{t}(x-y) + \frac{x}{\|x\|} = 0 \\
            y = & \; x\left(1 + \frac{t}{\|x\|}\right) \implies x = \alpha y \text{ for some } \alpha \in \R_{++} \text{ as } t > 0 \\
            y = & \; \alpha y + \frac{t\alpha y}{\alpha\|y\|} \\ 
            \alpha = & \; 1 - \frac{t}{\|y\|} \text{, so:} \\
            \prox_{tf}(y) = & \; x = \alpha y = \left(1 - \frac{t}{\|y\|}\right)y
        \end{split}
    \end{equation}
    \qed
\end{enumerate}

\end{homeworkProblem}

\textbf{The coding assignment is in .ipynb file attached to this homework}.

\end{document}
